# -*- coding: utf-8 -*-
"""MLP15

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/185DQ8qM27v_tRu3GeTlZKroGCXHCWedW
"""

!pip install tensorflow numpy pandas scikit-learn

# Google Drive 마운트
from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import time

# データ読み込み
default_folder = '/content/drive/MyDrive/2024kenkyu/simulation1'
# training_file = os.path.join(default_folder, 'traininginput.csv') #
training_file = os.path.join(default_folder, 'originpath.csv') #
test_file = os.path.join(default_folder, 'testinput.csv')

def train_and_evaluate_model(X, y, epochs, model_name):
    input_nodes = 12
    hidden_nodes = 7
    hidden_layers = 5
    output_nodes = 6
    learning_rate = 0.001

    # 訓練データ・検証データに分割
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)

    # NaN 値は平均値として入れる
    X_train = X_train.fillna(X_train.mean())
    X_val = X_val.fillna(X_train.mean())

    # データ正規化
    scaler = MinMaxScaler()
    X_train_normalized = scaler.fit_transform(X_train)
    X_val_normalized = scaler.transform(X_val)

    # MLPモデルの生成
    model = MLPRegressor(
        hidden_layer_sizes=(hidden_nodes,) * hidden_layers,
        activation='relu',
        solver='adam',
        learning_rate_init=learning_rate,
        max_iter=10,
        random_state=42,
        warm_start=True
    )

    # 1エポックあたりの損失を記録
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        # モデル訓練
        model.fit(X_train_normalized, y_train)

        # 訓練データによる予測と損失計算
        y_train_pred = model.predict(X_train_normalized)
        train_loss = mean_squared_error(y_train, y_train_pred)
        train_losses.append(train_loss)

        # 検証データによる予測と損失計
        y_val_pred = model.predict(X_val_normalized)
        val_loss = mean_squared_error(y_val, y_val_pred)
        val_losses.append(val_loss)

        # print(f"{model_name} - Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}")

    # 学習曲線を描く
    plt.plot(range(1, epochs + 1), train_losses, label=f'{model_name} - Train Loss')
    plt.plot(range(1, epochs + 1), val_losses, label=f'{model_name} - Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Mean Squared Error')
    plt.legend()
    plt.show()

    return model

# Training Data 読み込み
df = pd.read_csv(training_file)

# 変数定義
models = []
X = [None] * 15
y_col = [[]]
y = []

# columsを定義
s_col = ['s' + letter for letter in 'abcdef']
g_col = ['g' + letter for letter in 'abcdef']
y_col = [['m' + str(i) + letter for letter in 'abcdef'] for i in range(1, 16)]

# y 定義
for i in range(0, 15):
    y.append(df[y_col[i]])

# X 定義
X[0] = df[s_col + g_col]
X[1] = df[s_col + y_col[0]]
X[2] = df[y_col[0] + g_col]
X[3] = df[s_col + y_col[1]]
X[4] = df[y_col[1] + y_col[0]]
X[5] = df[y_col[0] + y_col[2]]
X[6] = df[y_col[2] + g_col]
X[7] = df[s_col + y_col[3]]
X[8] = df[y_col[3] + y_col[1]]
X[9] = df[y_col[1] + y_col[4]]
X[10] = df[y_col[4] + y_col[0]]
X[11] = df[y_col[0] + y_col[5]]
X[12] = df[y_col[5] + y_col[2]]
X[13] = df[y_col[2] + y_col[6]]
X[14] = df[y_col[6] + g_col]

models = {}  # モデルをディクショナリに入れる
# epochs = [20, 15, 22, 15, 18, 21, 22, 13, 15, 18, 20, 22, 22, 25, 25] #simu1
# epochs = [20, 35, 12, 5, 40, 35, 30, 5, 30, 25, 40, 45, 28, 20, 35] #simu2
epochs = 10

for i in range(0, 15):

    # モデル名
    model_name = 'Model' + str(i)

    # 訓練
    model = train_and_evaluate_model(X[i], y[i], epochs, model_name=model_name)
    # model = train_and_evaluate_model(X[i], y[i], epochs[i], model_name=model_name)

    # モデルをディクショナリに入れる
    models[model_name] = model
    print(f"{model_name} trained.")

# 学習が終わったらプリント
print("All models trained.")

# テストデータ読み込み
test_df = pd.read_csv(test_file)

# テストデータをX，ｙに入れる
t_X = [None] * 15
t_y = []

# t_X, t_yを定義
for i in range(0, 15):
    t_y.append(test_df[y_col[i]])

t_X[0] = test_df[s_col + g_col]
t_X[1] = test_df[s_col + y_col[0]]
t_X[2] = test_df[y_col[0] + g_col]
t_X[3] = test_df[s_col + y_col[1]]
t_X[4] = test_df[y_col[1] + y_col[0]]
t_X[5] = test_df[y_col[0] + y_col[2]]
t_X[6] = test_df[y_col[2] + g_col]
t_X[7] = test_df[s_col + y_col[3]]
t_X[8] = test_df[y_col[3] + y_col[1]]
t_X[9] = test_df[y_col[1] + y_col[4]]
t_X[10] = test_df[y_col[4] + y_col[0]]
t_X[11] = test_df[y_col[0] + y_col[5]]
t_X[12] = test_df[y_col[5] + y_col[2]]
t_X[13] = test_df[y_col[2] + y_col[6]]
t_X[14] = test_df[y_col[6] + g_col]

# テストを行う
losses = []
predictions = []

for i, (model_name, model) in enumerate(models.items()):
    print(f"Testing {model_name}:")

    # テストデータ正規化
    scaler = MinMaxScaler()
    scaler.fit(t_X[i])
    t_X_normalized = scaler.transform(t_X[i])

    # テストデータによる予測
    t_y_pred = model.predict(t_X_normalized)

    # 予測値と実際の値を比較
    t_loss = mean_squared_error(t_y[i], t_y_pred)
    print(f"Test Loss for t_X[{i}]: {t_loss}")

    # 結果をセーブ
    prediction_df = pd.DataFrame(t_y_pred, columns=y_col[i])

    # 結果を出力
    print(f"Predictions for t_X[{i}]:\n{prediction_df}")

    #　結果をリストに追加
    predictions.append(prediction_df)

    # 損失値を収集
    losses.append(t_loss)

# パス生成にかかる時間測定
# 評価時に必要であれば実行する
individual_times = []
times = []
for j in range(100) :
  start_time = time.time()
  for i, (model_name, model) in enumerate(models.items()):
      # 正規化
      scaler = MinMaxScaler()
      scaler.fit(t_X[i])
      t_X_normalized = scaler.transform(t_X[i])

      # 予測
      t_y_pred = model.predict(t_X_normalized)
  end_time = time.time()
  individual_times.append(end_time - start_time)
  times.append(individual_times)
avg_time = np.mean(times[i])
var_time = np.var(times[i])
print(f"Average time: {avg_time}")
print(f"Variance of time: {var_time}")

# パスの順番に合わせて結果を結合
desired_order = [7, 3, 8, 1, 9, 4, 10, 0, 11, 5, 12, 2, 13, 6, 14]
ordered_predictions = [predictions[i] for i in desired_order]
all_predictions = pd.concat(ordered_predictions, axis=1)

# s_col, all_predictions, g_colを合わせて新しいデータフレームに入れる
all_combined = pd.concat([test_df[s_col], all_predictions, test_df[g_col]], axis=1)

# 結果をcsvで出力
all_combined.to_csv(f"{default_folder}/after_train.csv", index=False) # all path