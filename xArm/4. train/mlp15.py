# -*- coding: utf-8 -*-
"""MLP15

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/185DQ8qM27v_tRu3GeTlZKroGCXHCWedW
"""

!pip install tensorflow numpy pandas scikit-learn

# Google Drive 마운트
from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import time

# 데이터 로드
default_folder = '/content/drive/MyDrive/2024kenkyu/simulation1'
training_file = os.path.join(default_folder, 'traininginput.csv')
test_file = os.path.join(default_folder, 'testinput.csv')

def train_and_evaluate_model(X, y, epochs, model_name):
    input_nodes = 12
    hidden_nodes = 30
    hidden_layers = 12
    output_nodes = 6
    learning_rate = 0.001

    # 데이터 분할
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)

    # NaN 값 평균으로 대체
    X_train = X_train.fillna(X_train.mean())
    X_val = X_val.fillna(X_train.mean())

    # 데이터 정규화
    scaler = MinMaxScaler()
    X_train_normalized = scaler.fit_transform(X_train)
    X_val_normalized = scaler.transform(X_val)

    # MLP 모델 생성
    model = MLPRegressor(
        hidden_layer_sizes=(hidden_nodes,) * hidden_layers,
        activation='relu',
        solver='adam',
        learning_rate_init=learning_rate,
        max_iter=10,
        random_state=42,
        warm_start=True
    )

    # 에폭별로 손실 기록
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        # 모델 훈련
        model.fit(X_train_normalized, y_train)

        # 훈련 데이터에 대한 예측과 손실 계산
        y_train_pred = model.predict(X_train_normalized)
        train_loss = mean_squared_error(y_train, y_train_pred)
        train_losses.append(train_loss)

        # 검증 데이터에 대한 예측과 손실 계산
        y_val_pred = model.predict(X_val_normalized)
        val_loss = mean_squared_error(y_val, y_val_pred)
        val_losses.append(val_loss)

        # print(f"{model_name} - Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}")

    # 학습 곡선 그리기
    plt.plot(range(1, epochs + 1), train_losses, label=f'{model_name} - Train Loss')
    plt.plot(range(1, epochs + 1), val_losses, label=f'{model_name} - Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Mean Squared Error')
    plt.legend()
    plt.show()

    return model

# Training Data 불러오기
df = pd.read_csv(training_file)

# 반복문 변수 정의
models = []
X = [None] * 15
y_col = [[]]
y = []

# X, y의 정의
# colums 정의
s_col = ['s' + letter for letter in 'abcdef']
g_col = ['g' + letter for letter in 'abcdef']
y_col = [['m' + str(i) + letter for letter in 'abcdef'] for i in range(1, 16)]

# y 정의
for i in range(0, 15):
    y.append(df[y_col[i]])

# X 정의
X[0] = df[s_col + g_col]
X[1] = df[s_col + y_col[0]]
X[2] = df[y_col[0] + g_col]
X[3] = df[s_col + y_col[1]]
X[4] = df[y_col[1] + y_col[0]]
X[5] = df[y_col[0] + y_col[2]]
X[6] = df[y_col[2] + g_col]
X[7] = df[s_col + y_col[3]]
X[8] = df[y_col[3] + y_col[1]]
X[9] = df[y_col[1] + y_col[4]]
X[10] = df[y_col[4] + y_col[0]]
X[11] = df[y_col[0] + y_col[5]]
X[12] = df[y_col[5] + y_col[2]]
X[13] = df[y_col[2] + y_col[6]]
X[14] = df[y_col[6] + g_col]

models = {}  # 모델을 저장하기 위한 딕셔너리
epochs = [9, 18, 18, 15, 28, 8, 16, 10, 25, 30, 30, 45, 25, 18, 10] #simu1
# epochs = [20, 35, 12, 5, 40, 35, 30, 5, 30, 25, 40, 45, 28, 20, 35] #simu2
# epochs = 50

for i in range(0, 15):

    # 모델 이름 정의
    model_name = 'Model' + str(i)

    # 학습
    # model = train_and_evaluate_model(X[i], y[i], epochs, model_name=model_name)
    model = train_and_evaluate_model(X[i], y[i], epochs[i], model_name=model_name)

    # 모델을 딕셔너리에 저장
    models[model_name] = model
    print(f"{model_name} trained.")

# 모든 모델 학습 완료
print("All models trained.")

# 테스트 데이터 로드
test_df = pd.read_csv(test_file)

# 테스트 데이터 분할
t_X = [None] * 15
t_y = []

# t_X, t_y의 정의
for i in range(0, 15):
    t_y.append(test_df[y_col[i]])

t_X[0] = test_df[s_col + g_col]
t_X[1] = test_df[s_col + y_col[0]]
t_X[2] = test_df[y_col[0] + g_col]
t_X[3] = test_df[s_col + y_col[1]]
t_X[4] = test_df[y_col[1] + y_col[0]]
t_X[5] = test_df[y_col[0] + y_col[2]]
t_X[6] = test_df[y_col[2] + g_col]
t_X[7] = test_df[s_col + y_col[3]]
t_X[8] = test_df[y_col[3] + y_col[1]]
t_X[9] = test_df[y_col[1] + y_col[4]]
t_X[10] = test_df[y_col[4] + y_col[0]]
t_X[11] = test_df[y_col[0] + y_col[5]]
t_X[12] = test_df[y_col[5] + y_col[2]]
t_X[13] = test_df[y_col[2] + y_col[6]]
t_X[14] = test_df[y_col[6] + g_col]

# 테스트 단계
losses = []
predictions = []
for i, (model_name, model) in enumerate(models.items()):
    print(f"Testing {model_name}:")

    # 테스트 데이터 정규화
    scaler = MinMaxScaler()
    scaler.fit(t_X[i])
    t_X_normalized = scaler.transform(t_X[i])

    # 테스트 데이터에 대한 예측
    t_y_pred = model.predict(t_X_normalized)

    # 예측값과 실제값 비교
    t_loss = mean_squared_error(t_y[i], t_y_pred)
    print(f"Test Loss for t_X[{i}]: {t_loss}")

    # 예측 결과를 DataFrame으로 저장
    prediction_df = pd.DataFrame(t_y_pred, columns=y_col[i])

    # 예측 결과 출력
    print(f"Predictions for t_X[{i}]:\n{prediction_df}")

    # 예측 결과를 리스트에 추가
    predictions.append(prediction_df)

    # 테스트 손실 값 수집
    losses.append(t_loss)

# 모든 테스트에 대한 예측 결과를 DataFrame으로 결합
all_predictions = pd.concat(predictions, axis=1)

# s_col, all_predictions, g_col을 결합하여 새로운 DataFrame 생성
all_combined = pd.concat([test_df[s_col], all_predictions, test_df[g_col]], axis=1)

# 결합된 DataFrame을 CSV 파일로 저장
all_combined.to_csv(f"{default_folder}/all_data.csv", index=False)